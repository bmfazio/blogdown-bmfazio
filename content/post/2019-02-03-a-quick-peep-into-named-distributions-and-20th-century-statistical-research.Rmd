---
title: A quick peep into named distributions and 20th century statistical research
author: Boris Fazio
date: '2019-02-03'
slug: a-quick-peep-into-named-distributions-and-20th-century-statistical-research
categories:
  - hmm
tags:
  - statistics
---

```{r, include=FALSE}
library(knitr)
library(tidyverse)
library(kableExtra)

kformat <- function(x){
  x %>%
    column_spec(1:2, width = "6em") %>%
    add_header_above(c(" " = 2, "Expected frequencies" = 3)) %>%
    kable_styling(position = "center")
}
```


I was looking at the documentation for `brms::brm` and saw the term 'distributional regression' which, albeit I had already been using in practice, I'd never heard of before.

This post is going to be about a niche lineage of papers I stumbled upon and my very opinionated take on it which, considering my current level of statistical knowledge, will in all probability be Quite Bad. First, an explanation for how I got there.

## Some context

I'm currently doing my thesis on the endpoint-inflated binomial model. Building the actual model and fitting it under a Bayesian framework is pretty straightforward, but I've struggled a fair bit with finding practical use cases.^[Explaining the circumstances under which a statistical model can be considered useful deserves a post of its own, Galit Shmueli's [*To Explain or to Predict?*](https://projecteuclid.org/euclid.ss/1294167961) is not a bad place to start]

An interesting property of probability distributions is that distinct processes may result in the same formulas.^[There's definitely a larger point that can be made here about how different sequences of steps can take you to the same place, but I'm not sure if it's actually insightful and I can't even articulate it anyway] In the particular case of the endpoint-inflated binomial, Tian^[[*Generalized endpoint-inflated binomial model*, 2015](http://doi.org/10.1016/j.csda.2015.03.009)] lays out six processes^[They call them "stochastical representations", a term which I enjoy as it feels concise and intuitive] under which the distribution can arise.

>A discrete random variable that takes integer values between *0* and some known maximum *m* will have an endpoint-inflated binomial distribution if it can be modeled as one of the following:
>
> 1. Mixture of *degenerate(0)*, *degenerate(m)* and *binomial*
>
> 1. Mixture of *m\*Bernoulli* and *binomial*
>
> 1. Mixture of *degenerate(m)* and *zero-inflated binomial*
>
> 1. Mixture of *degenerate(0)*, *degenerate(m)* and *zero-truncated binomial*
>
> 1. Mixture of *degenerate(0)*, *degenerate(m)* and *maximum-truncated binomial*^[This one seems a bit redundant since you can achieve the same result by flipping the outcome coding for the previous one]
>
> 1. Mixture of *degenerate(0)*, *degenerate(m)* and *endpoint-truncated binomial*

Situations where the first process applies are relatively simple to picture. For example, the number of insects killed by some treatment, where *0* and *m* correspond to extremes on a susceptibility spectrum and the binomial distribution corresponds to a partially susceptible population. The rest of them... I've wracked my brain trying to come up with examples that didn't seem unrealistically convoluted and got nowhere.

Then I remembered that we're allowed to ask the internet for questions.

## Into the rabbit hole

I searched for *zero truncated distribution*^[In that moment of intellectual stupor experienced when trying to grapple with a new set of concepts, the truncated distributions seemed like the more complicated aspect in the set of stochastic representations offered. But they're actually quite direct in their meaning: they occur when the data generating mechanism prevents us from observing the truncated values. An example is length of stay for hotel guests, who must stay for at least one day by definition. This whole post might not have happened if I'd thought of such a scenario earlier.] and a paper by Ghitany *et al.*^[[*Zero-truncated Poisson–Lindley distribution and its application*, 2008](https://doi.org/10.1016/j.matcom.2007.11.021)] showed up with a promising abstract:

> The zero-truncated Poisson–Lindley distribution is introduced and investigated. In particular, the method of moments and maximum likelihood estimators of the distribution’s parameter are compared in small and large samples. **Application of the model to real data is given.**

Emphasis mine. Omitting some references to tables and hypothesis test calculations, the application section is very brief:

> Cullen et al. gave counts of sites with 1, 2, 3, 4 and 5 particles from immunogold assay data. The counts were 122, 50, 18, 4, 4.
>
>[...]
>
>We are interested in testing the null hypothesis H0: "Number of attached particles is a ZTPL random variable" versus the alternative hypothesis H1: "Number of attached particles is not a ZTPL random variable".
>
>[...]
>
>It follows that the null hypothesis H0 cannot be rejected; indeed, the close agreement between the observed and expected frequencies suggests that the ZTPL distribution provides a "good fit".

I didn't like this section for a few reasons:

1. No attempt was made at establishing a connection between the data generating process and the distribution under examination.

1. Using the NHST paradigm with such a small dataset basically guarantees the null won't be rejected, which happens to be a desirable result for an author wishing to show "good fit".

1. The model wasn't compared against any alternative, so we don't even know if this model is any better than whatever came before it.

These observations provided a starting point for some interesting reflections, but I'll save that for the end. First, I had to investigate further into the origins and presumed usefulness of the distribution posited in the paper.

## Following the trail

I'd previously come across another named distribution^[By this I mean a distribution named after someone. While not a relevant category in any statistical sense, I have a hunch that these are more likely to have been created with a specialized purpose in mind and, by virtue of being traceable back to a single individual's work, can provide real examples for budding statisticians who wish to understand how subject knowledge and random variation come together and give rise to new models (as opposed to the all-too-common practice of seeing which model from some preexisting collection gives the best fit, see [this excellent article](https://www.johndcook.com/blog/2010/08/11/what-distribution-does-my-data-have/) from John D. Cook on the matter).], [the Birnbaum-Saunders](https://en.wikipedia.org/wiki/Birnbaum%E2%80%93Saunders_distribution), which was created with a specific application in mind. I thought that perhaps Ghitany's paper had glossed over the applications because it was assumed that people who would care about that paper would already understand the distribution's relevance, so I decided to delve deeper and understand what this Lindley distribution being referenced was about.

It turns out there was a previous paper from that same year by Ghitany^[[*Lindley distribution and its application*, 2008](https://doi.org/10.1016/j.matcom.2007.06.007)] which was solely about the Lindley distribution and also featured a positively succinct abstract:

>A treatment of the mathematical properties is provided for the Lindley distribution. The properties studied include: moments, cumulants, characteristic function, failure rate function, mean residual life function, mean deviations, Lorenz curve, stochastic ordering, entropies, asymptotic distribution of the extreme order statistics, distributions of sums, products and ratios, maximum likelihood estimation and simulation schemes. **An application to waiting time data at a bank is described.**

Let's take a look at that application:

>In this section, we use a real data set to show that the Lindley distribution can be a better model than one based on the exponential distribution. The data set given in Table 4 represents the waiting times (in minutes) before service of 100 bank customers.
>
>We fitted both the Lindley and exponential distributions to this data set. The method of maximum likelihood was used. We obtained the estimates $\theta = 0.187$ with $\text{S.E.}(\theta) = 0.013$ for the Lindley distribution and $\theta = 0.101$ with $\text{S.E.}(\theta) = 0.010$ for the exponential distribution. The maximized log-likelihoods for the two models were −319.0 and −329.0, respectively. Since the two models have the same number of parameters, it follows that the Lindley distribution provides the better fit.

Things to note here:

1. As opposed to the other paper, we do have a comparison between the author's distribution of interest and a more established one.

1. There's no reference given for the data. If you go to the table they reference, it's literally just a 10x10 array of numbers, which the caption says represent waiting times of bank customers.

1. Again, no attempt is made to justify the connection between the distribution of interest and the data being modeled, although there seems to be an implication that because it has a similar shape to the exponential^[Which does have a history of being used to model waiting times, with reasonable motivation for the choice, see [this question on StackExchange](https://math.stackexchange.com/questions/146293/why-do-we-always-assume-waiting-time-has-exponential-distribution)], it could also be suitable for the task.

Again I found this unsatisfactory, so down one level I went, this time to much older work from Sankaran^[[*275. Note: The Discrete Poisson-Lindley Distribution*, 1970](https://dx.doi.org/10.2307/2529053)]:

> A compound Poisson distribution is obtained by compounding the Poisson distribution with one due to Lindley. Estimation of the parameter is discussed, examples are given of the fitting of this distribution to data, and the fit is compared with that obtained using other distributions.

The article essentially consists of one page where the distribution and related formulas are presented, followed by two applications presented in tables which I transcribe here with the corresponding text:

>We consider two examples of observed data for which the two-parameter Hermite and negative binomial distributions, respectively, have been fitted. Tables 1 and 2 give the comparison of observed and expected frequencies for these, the Poisson, and the Poisson-Lindley distributions. Maximum likelihood was used to fit the Hermite distribution, but moment estimates were used in fitting the negative binomial and the present distribution.
>
>The present single-parameter distribution appears to give a satisfactory fit in both cases, whereas the Poisson distribution does not. For the two sets of data, $\{d \log L/d\theta\}_{\theta*}$ has the values 0.05 and 0.07 respectively, the second derivative being negative when $\theta = \theta*$, indicating that the moment estimate $\theta*$ is close to the maximum likelihood estimate in both cases.

```{r, echo = FALSE}
tibble(
  `No. of errors per group` = 0:4,
  `Observed frequencies` = c(35, 11, 8, 4, 2),
  `Poisson` = c(27.4, 21.5, 8.4, 2.2, 0.4),
  `Hermite` = c(34.2, 11.7, 9.6, 2.8, 1.3),
  `Poisson-Lindley` = c(33.0, 15.3, 6.8, 2.9, 1.2)
) %>% 
  kable("html", caption = "Distribution of mistakes in copying groups of random digits. Data from Kemp and Kemp (1965).") %>%
  kformat
```

```{r, echo = FALSE}
tibble(
  `No. of errors per group` = c(0:4, ">=5"),
  `Observed frequencies` = c(447, 132, 42, 21, 3, 2),
  `Poisson` = c(406, 189, 45, 7, 1, 0.1),
  `Hermite` = c(442, 140, 45, 14, 5, 2),
  `Poisson-Lindley` = c(441, 143, 45, 14, 4, 1)
) %>% 
  kable("html", caption = "Accidents to 647 women working on high explosive shells in 5 weeks. Data from Greenwood and Yule (1920) reported by Kendall and Stuart (1963, p. 129).") %>%
  kformat
```

Once again it seems that the appropriateness of the distribution to the data at hand is assumed obvious. It's also interesting to note that, while the Poisson fit performs the worst in both cases, the proposed distribution isn't the best in any, either (at least if we go by the distance between expected and observed counts).

At this point, the only thing left to do was check out the paper by Lindley^[[*Fiducial Distributions and Bayes' Theorem*, 1958](https://www.jstor.org/stable/2983909)] himself.

## So that's where you came from

In keeping with the pattern of the post, here's the paper's abstract:

>$x$ is a one-dimensional random variable whose distribution depedns on a single parameter $\theta$. It is the purpose of this note to establish two results:
>
>(i) The necessary and sufficient condition for the fiducial distribution of $\theta$, given $x$, to be a Bayes' distribution is that there exist transformations of $x$ to $u$, and of $\theta$ to $\tau$, such that $\tau$ is a location parameter for $u$. The condition will be referred to as (A). This extends some results of Grundy's (1956).
>
>(ii) If, for a random sample of any size from the distribution for $x$, there exists a single sufficient statistic for $\theta$ then the fiducial argument is inconsistent unless condition (A) obtains: and when it does, the fiducial argument is equivalent to a Bayesian argument with uniform prior distribution for $\tau$.
>
>The note concludes with an investigation of (A) in th case of the exponential family.

I have a very superficial understanding of fiducial inference and the term "Bayes' distribution" tripped me up at first, but I believe Lindley's intention here was to show in a fairly general setting that the fiducial framework offered nothing above and beyond a Bayesian one.

In this specific context, *consistency* of the fiducial argument means that it should result in the same inference as that obtained through Bayesian means when the same information is provided to both methods. Lindley tells us that for that to be the case, a posterior distribution that takes prior information encoded in a sufficient statistic $x$ and updates it with data that has a sufficient statistic $y$ should be equal to the fiducial distribution produced given $x$ and $y$ as data.

I did not read through the specifics, but what is purportedly shown in the end is that fiducial consistency is only obtained when a Bayesian would set up their priors in a certain specific way. Therefore, the implication here is that you could just be Bayesian and reach the same conclusions, with the added flexibility of being able to specify different priors if needed.

Along the way, Lindley gives an example of a distribution that leads to an inconsistency with the fiducial approach. Funny thing is that the distribution simply conjured on the spot, with a form explicitly chosen for simplicity. 

What?!

## Epilogue

I'll be honest, I find it quite amusing that some arbitrary distribution Lindley came up with on the spot ended up spawning a lineage of publications spanning over half a century^[There are more recent publications referencing the distribution which I didn't include here.]. What I don't find quite as amusing is what I perceive as an unwarranted attempt at pushing a "real world" utility of that distribution.

I've had the opportunity of meeting pure mathematicians. The perhaps all too stereotyped but quite real kind of person that devotes much of their life to understanding the properties of logical constructs with no clear connection to our real world for the sheer pleasure of it, much like professional puzzle solvers. I respect what they do, and I've heard of cases where long-solved math problems ended up providing answers to modern problems in applied areas, but even without any guarantee of the sorts, it is important that we give a place to such pursuers of intellectual freedom in our society. I will defer to the great Richard McElreath for [further and better presented justification](http://elevanth.org/blog/2018/09/02/golden_eggs/).

In this particular case, however, I feel that what could very well have been a pleasant exploration of the statistical properties of an arbitrary distribution is marred by those repeated and poorly laid out attempts at pushing some ill-defined sense of "real world utility". I think it unecessary and dishonest, though perhaps it's a good moment to bring up the Gelman twist on Clarke's Law:

> [Any sufficiently crappy research is indistinguishable from fraud](https://statmodeling.stat.columbia.edu/2016/06/20/clarkes-law-of-research/)

To be clear, only the recurrent application sections in that lineage of papers is what might deserve the "crappy" label. The rest of each of those articles seems to follow a reasonable if formulaic format for showing aspects of interest to mathematical statisticians, though it all mostly goes over my head.

Precisely because of the effort that seemingly went into the rest of the work, it is quite likely that such a throwaway treatment of applications was the product of a particular academic praxis rather than straight up deceitful attempts at overinflating the significance of their work.

## Final remarks

Overall, I feel that this has been a productive exploration. The differences between the concerns exhibited in those papers and the ones I've become used to reading (and caring about) has allowed me to better appreciate intra-field variation at a time where some seem to regard the whole of statistics as this one single thing you must be good at for certain jobs.

Also, the whole process of finding, reading the papers and writing this post took me almost three days. To be fair, I also spent some of that time figuring out formatting as I'm still new to `blogdown`. Just in case anyone wants to know^[I probably will in the future when I again feel inefficient and come looking for some benchmark against past me].